{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build datasets from FASHION_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_worker: 4\n",
      "\n",
      "[[ 3.6774972e-03  1.1755139e-02 -5.4557086e-03 ...  2.0792931e-03\n",
      "  -2.3649285e-02  9.5707532e-03]\n",
      " [-8.4031123e-04  6.1789686e-03  1.0210714e-02 ...  3.0820500e-03\n",
      "  -2.3839362e-02  8.0350682e-04]\n",
      " [-1.0348832e-02  2.2596184e-03 -5.3320164e-03 ...  2.7553646e-03\n",
      "   2.3408558e-03 -1.5501755e-02]\n",
      " ...\n",
      " [-9.3191833e-05 -3.9124326e-03  1.2919210e-03 ... -5.0274194e-03\n",
      "  -4.7465544e-03 -9.8960726e-03]\n",
      " [-9.8233540e-03 -1.6794683e-02  1.5876664e-02 ... -4.8428690e-03\n",
      "  -3.2504566e-03  9.9565303e-03]\n",
      " [ 8.9454530e-03 -2.0942942e-03  2.7219688e-03 ... -1.0475126e-02\n",
      "  -3.6723365e-04 -4.8272088e-03]]\n",
      "<NDArray 784x10 @cpu(0)> \n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "<NDArray 1x10 @cpu(0)>\n",
      "(784, 10) (1, 10)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import d2lzh as d2l\n",
    "from mxnet.gluon import data as gdata\n",
    "from mxnet import autograd, nd\n",
    "\n",
    "batch_size = 256\n",
    "transformer = gdata.vision.transforms.ToTensor()\n",
    "mnist_train, mnist_test = gdata.vision.FashionMNIST(train=True), gdata.vision.FashionMNIST(train=False)\n",
    "\n",
    "if sys.platform.startswith('win'):\n",
    "    num_workers = 1\n",
    "else:\n",
    "    num_workers = 4\n",
    "    \n",
    "####\n",
    "!echo num_worker: {num_workers} \n",
    "####\n",
    "\n",
    "train_iter, test_iter = \\\n",
    "    gdata.DataLoader(mnist_train.transform_first(transformer), batch_size, shuffle=True, num_workers=num_workers), \\\n",
    "    gdata.DataLoader(mnist_test.transform_first(transformer), batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "num_inputs, num_outputs = 28 * 28, 10 # each image's size = 28^2 pixel\n",
    "W = nd.random.normal(scale=0.01, shape=(num_inputs, num_outputs))\n",
    "b = nd.zeros((1, num_outputs))\n",
    "\n",
    "W.attach_grad()\n",
    "b.attach_grad()\n",
    "\n",
    "print(W, b)\n",
    "print(W.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[5. 7. 9.]]\n",
      "<NDArray 1x3 @cpu(0)> \n",
      "[[ 6.]\n",
      " [15.]]\n",
      "<NDArray 2x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "X = nd.array([[1, 2, 3],[4, 5, 6]])\n",
    "print(X.sum(axis=0, keepdims=True), X.sum(axis=1, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.7114856   0.2477707   1.414129   -0.594405    0.05316189]\n",
      " [-0.42178673 -1.0713187  -0.6094676   0.42622593 -0.55523616]]\n",
      "<NDArray 2x5 @cpu(0)> \n",
      "[ 0.40917096 -2.2315834 ]\n",
      "<NDArray 2 @cpu(0)>\n",
      "\n",
      "[[0.06552974 0.17101656 0.5490111  0.0736692  0.14077342]\n",
      " [0.17981592 0.09391608 0.14904566 0.4198705  0.15735182]]\n",
      "<NDArray 2x5 @cpu(0)> \n",
      "[1. 1.]\n",
      "<NDArray 2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "def softmax(X):\n",
    "    # X's rows = samples\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_exp / partition\n",
    "\n",
    "X = nd.random.normal(shape=(2, 5))\n",
    "\n",
    "print(X, X.sum(axis=1))\n",
    "X_prob = softmax(X)\n",
    "print(X_prob, X_prob.sum(axis=1))\n",
    "\n",
    "def net(X):\n",
    "#     print(X.reshape(-1, num_inputs).shape, W.shape)\n",
    "    return softmax(nd.dot(X.reshape(-1, num_inputs), W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.1 0.5]\n",
      "<NDArray 2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "y_hat = nd.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y = nd.array([0, 2], dtype='int32')\n",
    "print(nd.pick(y_hat, y))\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    return -nd.pick(y_hat, y).log()\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_net(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.astype('float32')\n",
    "        acc_sum = (net(X).argmax(axis=1) == y).sum().asscalar()\n",
    "        n += y.size\n",
    "        return acc_sum / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11328125"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_net(test_iter, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2134 0.5 0\n",
      "0.2134 0.5 0\n",
      "0.2134 0.5 0\n",
      "0.2134 0.5 0\n",
      "0.2134 0.5 0\n",
      "0.2134 0.5 0\n",
      "0.2134 0.5 0\n",
      "0.2134 0.5 0\n",
      "0.2134 0.5 0\n",
      "0.2134 0.5 0\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 0.1\n",
    "\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, trainer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = .0, .0, 0\n",
    "        with autograd.record():\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "        l.backward()\n",
    "        if trainer is None:\n",
    "            d2l.sgd(params, lr, batch_size)\n",
    "        else:\n",
    "            trainer.step(batch_size)\n",
    "        y = y.astype('int32')\n",
    "        train_l_sum += l.asscalar()\n",
    "        train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "        n += y.size\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
