{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build datasets from FASHION_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_worker: 4\n",
      "\n",
      "[[-0.01214079  0.02156406  0.01093822 ... -0.00806932  0.01376901\n",
      "   0.00205885]\n",
      " [ 0.00994352 -0.00235806  0.00298818 ... -0.00962973  0.00508051\n",
      "   0.00756173]\n",
      " [ 0.0168393   0.01257365  0.00131232 ... -0.00987804  0.00958589\n",
      "  -0.01497647]\n",
      " ...\n",
      " [ 0.01550311  0.01372548  0.00444446 ...  0.00035544 -0.01057525\n",
      "  -0.00585316]\n",
      " [-0.00245804 -0.0076688  -0.00301254 ...  0.01712018  0.01279332\n",
      "  -0.00793114]\n",
      " [-0.02800045 -0.01546722 -0.00802924 ...  0.0114329  -0.0185089\n",
      "  -0.00983216]]\n",
      "<NDArray 784x10 @cpu(0)> \n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "<NDArray 1x10 @cpu(0)>\n",
      "(784, 10) (1, 10)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import d2lzh as d2l\n",
    "from mxnet.gluon import data as gdata\n",
    "from mxnet import autograd, nd\n",
    "\n",
    "batch_size = 256\n",
    "transformer = gdata.vision.transforms.ToTensor()\n",
    "mnist_train, mnist_test = gdata.vision.FashionMNIST(train=True), gdata.vision.FashionMNIST(train=False)\n",
    "\n",
    "if sys.platform.startswith('win'):\n",
    "    num_workers = 1\n",
    "else:\n",
    "    num_workers = 4\n",
    "    \n",
    "####\n",
    "!echo num_worker: {num_workers} \n",
    "####\n",
    "\n",
    "train_iter, test_iter = \\\n",
    "    gdata.DataLoader(mnist_train.transform_first(transformer), batch_size, shuffle=True, num_workers=num_workers), \\\n",
    "    gdata.DataLoader(mnist_test.transform_first(transformer), batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "num_inputs, num_outputs = 28 * 28, 10 # each image's size = 28^2 pixel\n",
    "W = nd.random.normal(scale=0.01, shape=(num_inputs, num_outputs))\n",
    "b = nd.zeros((1, num_outputs))\n",
    "\n",
    "W.attach_grad()\n",
    "b.attach_grad()\n",
    "\n",
    "print(W, b)\n",
    "print(W.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[5. 7. 9.]]\n",
      "<NDArray 1x3 @cpu(0)> \n",
      "[[ 6.]\n",
      " [15.]]\n",
      "<NDArray 2x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "X = nd.array([[1, 2, 3],[4, 5, 6]])\n",
    "print(X.sum(axis=0, keepdims=True), X.sum(axis=1, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.16282491  0.6836102   1.6106696  -1.2642232   0.1389543 ]\n",
      " [ 0.6671155   0.214447    0.99384654  0.10874183  0.5353432 ]]\n",
      "<NDArray 2x5 @cpu(0)> \n",
      "[1.006186 2.519494]\n",
      "<NDArray 2 @cpu(0)>\n",
      "\n",
      "[[0.09168093 0.21373768 0.5401294  0.03047529 0.12397669]\n",
      " [0.22366177 0.14223297 0.31009105 0.12796557 0.19604862]]\n",
      "<NDArray 2x5 @cpu(0)> \n",
      "[1. 1.]\n",
      "<NDArray 2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "def softmax(X):\n",
    "    # X's rows = samples\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_exp / partition\n",
    "\n",
    "X = nd.random.normal(shape=(2, 5))\n",
    "\n",
    "print(X, X.sum(axis=1))\n",
    "X_prob = softmax(X)\n",
    "print(X_prob, X_prob.sum(axis=1))\n",
    "\n",
    "def net(X):\n",
    "#     print(X.reshape(-1, num_inputs).shape, W.shape)\n",
    "    return softmax(nd.dot(X.reshape(-1, num_inputs), W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.1 0.5]\n",
      "<NDArray 2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "y_hat = nd.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y = nd.array([0, 2], dtype='int32')\n",
    "print(nd.pick(y_hat, y))\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    return -nd.pick(y_hat, y).log()\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_net(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.astype('float32')\n",
    "        acc_sum = (net(X).argmax(axis=1) == y).sum().asscalar()\n",
    "        n += y.size\n",
    "        return acc_sum / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09375"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_net(test_iter, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.7878674438476563, train_acc_sum: 44816.0, test acc: 0.8020833134651184\n",
      "epoch: 1, loss: 0.5739248268127441, train_acc_sum: 48611.0, test acc: 0.8125\n",
      "epoch: 2, loss: 0.5293440882364909, train_acc_sum: 49331.0, test acc: 0.8541666865348816\n",
      "epoch: 3, loss: 0.5048197692235311, train_acc_sum: 49791.0, test acc: 0.875\n",
      "epoch: 4, loss: 0.4890014295578003, train_acc_sum: 50081.0, test acc: 0.8125\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 0.1\n",
    "\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, trainer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = .0, .0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            if trainer is None:\n",
    "                d2l.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "#           print(y_hat, y)\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = accuracy(y_hat, y)\n",
    "        print(f'epoch: {epoch}, loss: {train_l_sum / n}, train_acc_sum: {train_acc_sum}, test acc: {test_acc}')\n",
    "\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
