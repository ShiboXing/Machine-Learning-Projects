{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build datasets from FASHION_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_worker: 4\n",
      "\n",
      "[[ 0.00241127  0.0033418  -0.01314738 ... -0.00470703  0.01259373\n",
      "  -0.00720676]\n",
      " [ 0.00600226  0.00011061  0.0001107  ...  0.00323387 -0.00515725\n",
      "   0.0056391 ]\n",
      " [-0.00306754  0.01372907 -0.00487223 ... -0.00297172  0.00460683\n",
      "   0.00097664]\n",
      " ...\n",
      " [-0.01689546  0.00070881 -0.01083925 ... -0.00928763 -0.00952997\n",
      "  -0.00600304]\n",
      " [-0.00338771 -0.01121285 -0.01522656 ...  0.00986621 -0.00403203\n",
      "  -0.00121138]\n",
      " [-0.00856771  0.00302645 -0.00781122 ... -0.00780047  0.00519805\n",
      "  -0.01402952]]\n",
      "<NDArray 784x10 @cpu(0)> \n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "<NDArray 1x10 @cpu(0)>\n",
      "(784, 10) (1, 10)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import d2lzh as d2l\n",
    "from mxnet.gluon import data as gdata\n",
    "from mxnet import autograd, nd\n",
    "\n",
    "batch_size = 256\n",
    "transformer = gdata.vision.transforms.ToTensor()\n",
    "mnist_train, mnist_test = gdata.vision.FashionMNIST(train=True), gdata.vision.FashionMNIST(train=False)\n",
    "\n",
    "if sys.platform.startswith('win'):\n",
    "    num_workers = 1\n",
    "else:\n",
    "    num_workers = 4\n",
    "    \n",
    "####\n",
    "!echo num_worker: {num_workers} \n",
    "####\n",
    "\n",
    "train_iter, test_iter = \\\n",
    "    gdata.DataLoader(mnist_train.transform_first(transformer), batch_size, shuffle=True, num_workers=num_workers), \\\n",
    "    gdata.DataLoader(mnist_test.transform_first(transformer), batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "num_inputs, num_outputs = 28 * 28, 10 # each image's size = 28^2 pixel\n",
    "W = nd.random.normal(scale=0.01, shape=(num_inputs, num_outputs))\n",
    "b = nd.zeros((1, num_outputs))\n",
    "\n",
    "W.attach_grad()\n",
    "b.attach_grad()\n",
    "\n",
    "print(W, b)\n",
    "print(W.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[5. 7. 9.]]\n",
      "<NDArray 1x3 @cpu(0)> \n",
      "[[ 6.]\n",
      " [15.]]\n",
      "<NDArray 2x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "X = nd.array([[1, 2, 3],[4, 5, 6]])\n",
    "print(X.sum(axis=0, keepdims=True), X.sum(axis=1, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.14835548 -0.36234885 -0.94928247 -0.15361013 -0.39580846]\n",
      " [ 0.9750422   0.32427043  0.30911517 -0.2388004   0.40020448]]\n",
      "<NDArray 2x5 @cpu(0)> \n",
      "[-2.0094054  1.7698319]\n",
      "<NDArray 2 @cpu(0)>\n",
      "\n",
      "[[0.24802741 0.20024586 0.11134265 0.24672754 0.19365658]\n",
      " [0.34538856 0.18016952 0.1774596  0.10259892 0.19438337]]\n",
      "<NDArray 2x5 @cpu(0)> \n",
      "[1. 1.]\n",
      "<NDArray 2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "def softmax(X):\n",
    "    # X's rows = samples\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_exp / partition\n",
    "\n",
    "X = nd.random.normal(shape=(2, 5))\n",
    "\n",
    "print(X, X.sum(axis=1))\n",
    "X_prob = softmax(X)\n",
    "print(X_prob, X_prob.sum(axis=1))\n",
    "\n",
    "def net(X):\n",
    "#     print(X.reshape(-1, num_inputs).shape, W.shape)\n",
    "    return softmax(nd.dot(X.reshape(-1, num_inputs), W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.1 0.5]\n",
      "<NDArray 2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "y_hat = nd.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y = nd.array([0, 2], dtype='int32')\n",
    "print(nd.pick(y_hat, y))\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    return -nd.pick(y_hat, y).log()\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_net(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.astype('float32')\n",
    "        acc_sum = (net(X).argmax(axis=1) == y).sum().asscalar()\n",
    "        n += y.size\n",
    "        return acc_sum / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09765625"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_net(test_iter, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.7875408166885376, train_acc_sum: 44690.0, test acc: 0.7916666865348816\n",
      "epoch: 1, loss: 0.5733886915842692, train_acc_sum: 48663.0, test acc: 0.75\n",
      "epoch: 2, loss: 0.529137768236796, train_acc_sum: 49418.0, test acc: 0.8645833134651184\n",
      "epoch: 3, loss: 0.5048939123153686, train_acc_sum: 49835.0, test acc: 0.8541666865348816\n",
      "epoch: 4, loss: 0.48991118818918866, train_acc_sum: 50091.0, test acc: 0.8333333134651184\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 0.1\n",
    "\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, trainer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = .0, .0, 0\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            if trainer is None:\n",
    "                d2l.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "#             print(y_hat, y)\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = accuracy(y_hat, y)\n",
    "        print(f'epoch: {epoch}, loss: {train_l_sum / n}, train_acc_sum: {train_acc_sum}, test acc: {test_acc}')\n",
    "\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
