{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import sys\n",
    "import d2lzh_pytorch as d2l\n",
    "import time\n",
    "import ipdb\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 784]) Parameter containing:\n",
      "tensor([[-0.0266,  0.0253, -0.0125,  ...,  0.0015,  0.0146,  0.0248],\n",
      "        [ 0.0142, -0.0036,  0.0058,  ...,  0.0249, -0.0072, -0.0129],\n",
      "        [ 0.0197, -0.0090, -0.0299,  ...,  0.0334,  0.0340, -0.0047],\n",
      "        ...,\n",
      "        [-0.0348, -0.0148,  0.0310,  ...,  0.0089,  0.0098,  0.0019],\n",
      "        [ 0.0114,  0.0026, -0.0336,  ..., -0.0299,  0.0051, -0.0156],\n",
      "        [-0.0144, -0.0219,  0.0353,  ...,  0.0044, -0.0292,  0.0161]],\n",
      "       requires_grad=True)\n",
      "torch.Size([256]) Parameter containing:\n",
      "tensor([ 0.0353,  0.0182, -0.0095,  0.0108, -0.0207,  0.0168,  0.0134, -0.0084,\n",
      "        -0.0003,  0.0059,  0.0253,  0.0231,  0.0266, -0.0074,  0.0267, -0.0140,\n",
      "        -0.0137,  0.0249, -0.0217,  0.0208, -0.0098, -0.0224, -0.0312, -0.0142,\n",
      "        -0.0249,  0.0186,  0.0300, -0.0121, -0.0229,  0.0033,  0.0288,  0.0120,\n",
      "         0.0078, -0.0165,  0.0090, -0.0090,  0.0011, -0.0127, -0.0145, -0.0052,\n",
      "         0.0084,  0.0352,  0.0259, -0.0279,  0.0269,  0.0082, -0.0324, -0.0306,\n",
      "         0.0330, -0.0256,  0.0188,  0.0272, -0.0145, -0.0306, -0.0191,  0.0071,\n",
      "        -0.0101, -0.0343, -0.0046, -0.0079,  0.0080, -0.0246,  0.0151, -0.0044,\n",
      "        -0.0133,  0.0324, -0.0044,  0.0221, -0.0337, -0.0354,  0.0187,  0.0336,\n",
      "         0.0227, -0.0020, -0.0252,  0.0193,  0.0165,  0.0316,  0.0098,  0.0100,\n",
      "        -0.0295,  0.0144,  0.0065, -0.0241,  0.0325,  0.0146, -0.0101, -0.0240,\n",
      "        -0.0241,  0.0207,  0.0349,  0.0306,  0.0099,  0.0162,  0.0308, -0.0284,\n",
      "         0.0228,  0.0346, -0.0323, -0.0123,  0.0190, -0.0052,  0.0288, -0.0307,\n",
      "        -0.0091, -0.0314, -0.0004,  0.0190, -0.0285, -0.0352,  0.0039,  0.0303,\n",
      "         0.0200, -0.0202,  0.0264, -0.0186, -0.0247, -0.0281,  0.0270, -0.0342,\n",
      "        -0.0134, -0.0188, -0.0004, -0.0192, -0.0301, -0.0040, -0.0352,  0.0208,\n",
      "         0.0133, -0.0248, -0.0134, -0.0208, -0.0275,  0.0120,  0.0094,  0.0143,\n",
      "        -0.0141, -0.0224,  0.0290, -0.0273,  0.0058,  0.0356,  0.0071, -0.0222,\n",
      "         0.0090, -0.0324, -0.0093, -0.0159,  0.0289, -0.0036, -0.0266, -0.0199,\n",
      "         0.0157,  0.0127, -0.0164, -0.0161,  0.0038,  0.0338, -0.0211,  0.0119,\n",
      "         0.0030,  0.0335,  0.0020, -0.0264, -0.0241,  0.0055, -0.0194,  0.0108,\n",
      "         0.0050,  0.0014,  0.0011,  0.0157, -0.0075, -0.0149,  0.0115,  0.0224,\n",
      "         0.0018,  0.0225, -0.0074, -0.0305,  0.0265, -0.0046, -0.0123, -0.0122,\n",
      "        -0.0342, -0.0226,  0.0190, -0.0151,  0.0330, -0.0350,  0.0142, -0.0266,\n",
      "         0.0089,  0.0246, -0.0014,  0.0178, -0.0282,  0.0140, -0.0216, -0.0051,\n",
      "        -0.0089,  0.0082,  0.0319, -0.0349, -0.0010, -0.0086, -0.0171,  0.0065,\n",
      "         0.0071, -0.0214, -0.0022, -0.0028,  0.0104, -0.0206,  0.0151, -0.0276,\n",
      "         0.0054, -0.0201, -0.0029,  0.0333,  0.0299, -0.0196,  0.0308,  0.0292,\n",
      "         0.0123,  0.0321,  0.0237, -0.0069, -0.0090,  0.0002,  0.0119,  0.0131,\n",
      "        -0.0152, -0.0056,  0.0155, -0.0139,  0.0300, -0.0077, -0.0033, -0.0221,\n",
      "         0.0296, -0.0122, -0.0319,  0.0260, -0.0319, -0.0172, -0.0057,  0.0236,\n",
      "         0.0113,  0.0144, -0.0232,  0.0066,  0.0074,  0.0051,  0.0018, -0.0042],\n",
      "       requires_grad=True)\n",
      "torch.Size([10, 256]) Parameter containing:\n",
      "tensor([[ 0.0431, -0.0283,  0.0272,  ..., -0.0237,  0.0361, -0.0558],\n",
      "        [-0.0533,  0.0113,  0.0176,  ...,  0.0170, -0.0554,  0.0545],\n",
      "        [ 0.0090, -0.0580, -0.0018,  ..., -0.0334,  0.0126, -0.0228],\n",
      "        ...,\n",
      "        [-0.0469,  0.0205,  0.0234,  ...,  0.0538, -0.0350, -0.0306],\n",
      "        [-0.0376,  0.0478,  0.0507,  ..., -0.0444,  0.0107,  0.0528],\n",
      "        [ 0.0288,  0.0444,  0.0533,  ..., -0.0449,  0.0280, -0.0337]],\n",
      "       requires_grad=True)\n",
      "torch.Size([10]) Parameter containing:\n",
      "tensor([ 0.0362, -0.0371,  0.0052, -0.0608, -0.0130,  0.0100,  0.0372, -0.0169,\n",
      "        -0.0028,  0.0296], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "net = nn.Sequential(\n",
    "    d2l.FlattenLayer(),\n",
    "    nn.Linear(num_inputs, num_hiddens),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(num_hiddens, num_outputs)\n",
    ")\n",
    "\n",
    "for param in net.parameters():\n",
    "    print(param.shape, param)\n",
    "    init.normal_(param, mean=0, std=0.01)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = .0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            net.eval()\n",
    "            acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "            net.train()\n",
    "            n += y.shape[0]\n",
    "        net.eval()\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-6-e34b34e1ee01>\u001b[0m(11)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     10 \u001b[0;31m    \u001b[0mtrain_l_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 11 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m        \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> y\n",
      "tensor([3, 1, 0, 9, 0, 9, 0, 4, 8, 2, 5, 8, 9, 4, 2, 5, 0, 3, 0, 8, 6, 0, 1, 2,\n",
      "        7, 9, 8, 0, 6, 0, 2, 4, 1, 2, 4, 3, 8, 3, 0, 8, 0, 6, 0, 7, 0, 6, 6, 1,\n",
      "        9, 2, 4, 3, 3, 1, 4, 9, 8, 8, 7, 9, 3, 0, 1, 0, 9, 3, 4, 8, 3, 2, 2, 2,\n",
      "        3, 7, 1, 2, 8, 6, 2, 7, 2, 8, 0, 9, 3, 0, 5, 8, 8, 5, 0, 3, 9, 6, 9, 6,\n",
      "        8, 2, 0, 3, 8, 9, 8, 1, 7, 6, 8, 0, 3, 1, 5, 3, 1, 5, 8, 1, 9, 1, 5, 6,\n",
      "        1, 9, 7, 5, 0, 6, 0, 2, 1, 6, 5, 0, 2, 1, 0, 4, 1, 8, 0, 0, 3, 9, 5, 9,\n",
      "        7, 8, 5, 3, 2, 6, 9, 8, 7, 9, 0, 0, 1, 4, 4, 9, 1, 1, 0, 2, 2, 6, 3, 5,\n",
      "        0, 8, 5, 9, 9, 1, 5, 8, 2, 2, 9, 7, 7, 5, 7, 3, 9, 1, 3, 4, 2, 0, 7, 7,\n",
      "        5, 4, 5, 0, 2, 5, 0, 7, 7, 9, 1, 8, 3, 9, 0, 9, 4, 7, 0, 4, 5, 6, 4, 2,\n",
      "        9, 8, 9, 6, 9, 5, 6, 2, 6, 9, 3, 1, 2, 5, 0, 4, 3, 0, 0, 0, 0, 1, 5, 4,\n",
      "        5, 9, 7, 2, 5, 7, 5, 5, 0, 7, 4, 2, 7, 7, 5, 9])\n",
      "ipdb> y.shape\n",
      "torch.Size([256])\n",
      "ipdb> y_hat\n",
      "tensor([[ 0.0256,  0.0094, -0.0002,  ..., -0.0251, -0.0014, -0.0162],\n",
      "        [ 0.0118,  0.0039,  0.0044,  ..., -0.0177,  0.0163, -0.0189],\n",
      "        [ 0.0118, -0.0044,  0.0093,  ..., -0.0159,  0.0058, -0.0140],\n",
      "        ...,\n",
      "        [ 0.0078,  0.0112,  0.0145,  ..., -0.0223,  0.0117, -0.0077],\n",
      "        [ 0.0006,  0.0065,  0.0117,  ..., -0.0191,  0.0197, -0.0221],\n",
      "        [ 0.0212,  0.0029,  0.0113,  ..., -0.0090,  0.0150, -0.0284]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "ipdb> y_hat.shape\n",
      "torch.Size([256, 10])\n",
      "ipdb> l\n",
      "\u001b[1;32m      6 \u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      7 \u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      8 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      9 \u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     10 \u001b[0m    \u001b[0mtrain_l_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 11 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     12 \u001b[0m        \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     13 \u001b[0m        \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     14 \u001b[0m        \u001b[0;31m# clear grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     15 \u001b[0m        \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     16 \u001b[0m        \u001b[0;31m# back-prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "ipdb> 'l'\n",
      "'l'\n",
      "ipdb> `l`\n",
      "*** SyntaxError: invalid syntax\n",
      "ipdb> !l\n",
      "tensor(2.3035, grad_fn=<SumBackward0>)\n",
      "ipdb> !l.shape\n",
      "torch.Size([])\n",
      "ipdb> loss(y_hat, y)\n",
      "tensor(2.3035, grad_fn=<NllLossBackward>)\n",
      "ipdb> loss\n",
      "CrossEntropyLoss()\n",
      "ipdb> loss(y_hat, y)\n",
      "tensor(2.3035, grad_fn=<NllLossBackward>)\n",
      "ipdb> y_hatg\n",
      "*** NameError: name 'y_hatg' is not defined\n",
      "ipdb> y_hat\n",
      "tensor([[ 0.0256,  0.0094, -0.0002,  ..., -0.0251, -0.0014, -0.0162],\n",
      "        [ 0.0118,  0.0039,  0.0044,  ..., -0.0177,  0.0163, -0.0189],\n",
      "        [ 0.0118, -0.0044,  0.0093,  ..., -0.0159,  0.0058, -0.0140],\n",
      "        ...,\n",
      "        [ 0.0078,  0.0112,  0.0145,  ..., -0.0223,  0.0117, -0.0077],\n",
      "        [ 0.0006,  0.0065,  0.0117,  ..., -0.0191,  0.0197, -0.0221],\n",
      "        [ 0.0212,  0.0029,  0.0113,  ..., -0.0090,  0.0150, -0.0284]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "ipdb> y.shape y_hat.shape\n",
      "*** SyntaxError: invalid syntax\n",
      "ipdb> y.shape\n",
      "torch.Size([256])\n",
      "ipdb> y_hat.shape\n",
      "torch.Size([256, 10])\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\n",
    "num_epochs = 5\n",
    "n = 0\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_l_loss, train_acc_sum, n = 0, 0, 0\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y).sum()\n",
    "        # clear grad \n",
    "        optimizer.zero_grad()\n",
    "        # back-prop\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        train_l_loss += l.item()\n",
    "        train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        n += y.shape[0]\n",
    "        ipdb.set_trace()\n",
    "    print('epoch %d, loss: %.4f train acc: %.4f test acc: %.4f' % (e + 1, train_l_loss / n, train_acc_sum / n, evaluate_accuracy(test_iter, net)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
